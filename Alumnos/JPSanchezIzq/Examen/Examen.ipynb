{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas funciones útiles\n",
    "\n",
    "def cuadrados(x):\n",
    "    resultado=0\n",
    "    for i in range(len(x)):\n",
    "        resultado+=x[i]**2\n",
    "    return resultado\n",
    "\n",
    "def Grad(f, x0, h=1e-6, i=-1):\n",
    "    \"\"\"\n",
    "    Función que calcula el Grad de una función en un punto\n",
    "    \"\"\"\n",
    "    n = len(x0)\n",
    "    if i in range(n):\n",
    "        z = np.zeros(n)\n",
    "        z[i] = h/2\n",
    "        Grad = (f(x0 + z) - f(x0 - z))/h\n",
    "    else:\n",
    "        Grad = np.zeros(n)\n",
    "        for j in range(n):\n",
    "            z = np.zeros(n)\n",
    "            z[j] = h/2\n",
    "            Grad[j]= (f(x0 + z) - f(x0 - z))/h\n",
    "    return Grad\n",
    "\n",
    "def Hess(f, x0, h=1e-4, method = \"basic\"):\n",
    "    \"\"\"\n",
    "    Función que calcula la Hessiana  de una función en un punto. \n",
    "    f: función sobre la cual queremos calcular la hessiana.\n",
    "    x0: Punto sobre el cual queremos hacer el cálculo\n",
    "    h: nivel de precisión para hacer el cálculo\n",
    "    method: Método por el cual se quiere hacer puede ser: 'basic', 'grad', 'centered', 'gradCentered'\n",
    "    \"\"\"\n",
    "    n = len(x0)\n",
    "    Hess = np.matrix(np.zeros((n,n)))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            z_i = np.zeros(n)\n",
    "            z_j = np.zeros(n)\n",
    "            if j<= i :\n",
    "                z_i[i] = h\n",
    "                z_j[j] = h\n",
    "                if method == \"basic\":\n",
    "                    Hess[i,j] = ( f(x0 + z_j +z_i) - f(x0 + z_i ) - f(x0+z_j) +f(x0)) / (h**2)\n",
    "                    Hess[j,i] = Hess[i,j]\n",
    "                elif method == \"grad\":\n",
    "                    Hess[i,j] = (Grad(f,x0+z_j,h,i) - Grad(f,x0,h,i) + \\\n",
    "                                Grad(f,x0+z_i,h,j) - Grad(f,x0,h,j))/(2*h)\n",
    "                    Hess[j,i] = Hess[i,j]\n",
    "                elif method == \"centered\":\n",
    "                    if i==j:\n",
    "                        Hess[i,j] = (-f(x0+2*z_i) + 16*f(x0+z_i) - 30*f(x0)+\\\n",
    "                                    16*f(x0-z_i) - f(x0-2*z_i))  / (12*h**2)\n",
    "                        Hess[j,i] = Hess[i,j]\n",
    "                    else :\n",
    "                        Hess[i,j] = (f(x0+z_i+z_j) - f(x0 + z_i - z_j) - \\\n",
    "                                    f(x0 - z_i + z_j) + f(x0-z_i-z_j))/(4*h**2)\n",
    "                        Hess[j,i] = Hess[i,j]\n",
    "                elif method == \"gradCentered\":\n",
    "                        Hess[i,j] = (Grad(f,x0+z_j,h)[i] - Grad(f, x0-z_j,h)[i] + \\\n",
    "                                    Grad(f,x0+z_i,h)[j] - Grad(f,x0-z_i,h)[j])/(4*h)\n",
    "                        Hess[j,i] = Hess[i,j]\n",
    "    return Hess\n",
    "\n",
    "def f_o_c(f,x, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Función que calcula las condiciones de primer orden\n",
    "    \"\"\"\n",
    "    grad = np.array(Grad(f,x))\n",
    "    if np.dot(grad, grad) < tol:\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "def s_o_c(f, x0, tol=1e-15):\n",
    "    \"\"\"\n",
    "    Inserten aqui código para condiciones de segundo orden \n",
    "    \"\"\"\n",
    "    hess = Hess(f, x0, tol)\n",
    "    if np.all(np.linalg.eigvals(hess) > tol) :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "def condiciones_wolfe(f, x0, alpha, pk, c1=1e-4, c2 = 1e-2, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Función que evalúa las condiciones de wolfe para una alpha. \n",
    "    f:  función que optimizamos\n",
    "    x0: punto anterior un numpy.array\n",
    "    alpha: valor que cumplirá condiciones de wolfe. \n",
    "    pk: dirección de decenso un numpy.array\n",
    "    \"\"\"\n",
    "    grad = lambda alpha: Grad(f,x0+alpha*pk, tol)\n",
    "    phi = lambda alpha: f(x0 + alpha*pk) # Ojo que phi(0) = f(x0)\n",
    "    linea = lambda alpha: phi(0) + c1 * alpha *np.dot( g_x0, pk)\n",
    "    g_x0 = grad(0) # grad(0) = Grad(f,x0)\n",
    "    cond_1 = linea(alpha) - phi(alpha) >= 0\n",
    "    cond_2 = np.dot(grad(alpha), pk) - c2 * np.dot(g_x0, pk) >=0\n",
    "    return  cond_1 and cond_2 \n",
    "\n",
    "\n",
    "def genera_alfa(f, x0, pk, c1=1e-4, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Backtracking LS i.e. Algoritmo que encuentra una alpha que cumpla condiciones de wolfe. \n",
    "    \"\"\"\n",
    "    alpha, rho, c = 1, 4/5, c1\n",
    "    while f(x0 + alpha*pk)>f(x0) + c*alpha*np.dot(Grad(f, x0),pk):\n",
    "        alpha*=rho\n",
    "    return alpha\n",
    "\n",
    "def is_pos_def(Hessiana):\n",
    "    \"\"\" Regresa True cuando la matriz es definida positiva \"\"\"\n",
    "    if np.all(np.linalg.eigvals(Hessiana) > 0):\n",
    "        return True\n",
    "\n",
    "def modificacion_hessiana(Hessiana, l = 0.5):\n",
    "    while not is_pos_def(Hessiana):\n",
    "        Hessiana = Hessiana + l*np.eye(len(Hessiana))\n",
    "    return Hessiana\n",
    "\n",
    "def busqueda_lineal(f, x0, method = \"Maximo descenso\" ):\n",
    "    xk = x0 \n",
    "    if method == \"Newton\" :\n",
    "        while not (f_o_c(f,xk) and s_o_c(f,xk)):\n",
    "            grad = Grad(f,xk)\n",
    "            hess = Hess(f,xk)\n",
    "            pk = np.linalg.solve(hess, -grad)\n",
    "            xk = xk + pk\n",
    "    else :\n",
    "        while not (f_o_c(f,xk) and s_o_c(f,xk)):\n",
    "            grad= Grad(f,xk)\n",
    "            pk = -grad\n",
    "            alpha = genera_alpha(f,xk, pk)\n",
    "            xk = xk + alpha*pk\n",
    "    return xk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1.1]\n",
      "[1, 1.2]\n"
     ]
    }
   ],
   "source": [
    "# Definimos la funcion de costo de Rosenbrok\n",
    "\n",
    "def Rosen(x0):\n",
    "    alfa = 10;\n",
    "    Rosen=(1-x0[0])**2 + alfa*(x0[1]-x0[0]**2)**2\n",
    "    return Rosen\n",
    "\n",
    "# b) metodo de newton \n",
    "\n",
    "def busqueda_lineal_Newton(f, x0):\n",
    "    xk = x0\n",
    "    while not (f_o_c(f,x0)) and (s_o_c(f,x0)):\n",
    "            grad=Grad(f,x0)\n",
    "            hess=Hess(f,x0)\n",
    "            pk=LA.solve(hess,-grad)\n",
    "            xk = xk + pk\n",
    "    return xk\n",
    "\n",
    "print(busqueda_lineal_Newton(Rosen,[1,1.1]))\n",
    "\n",
    "# c) busqueda lineal \n",
    "\n",
    "def busqueda_lineal(f, x0):\n",
    "    xk=x0\n",
    "    while not (f_o_c(f,xk)) and (s_o_c(f,xk)):\n",
    "            grad=Grad(f,xk)\n",
    "            pk = -grad\n",
    "            alpha = genera_alfa(f,xk,pk)\n",
    "            xk = xk + alpha*pk\n",
    "    return xk\n",
    "print( busqueda_lineal(Rosen,[1,1.2]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
